{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in', 'hers', 'ours', 'did', \"it's\", 'not', 'my', \"you've\", 'had', 'some', 'aren', 'where', 'such', 'with', 'it', 'but', \"hadn't\", \"that'll\", 'haven', \"don't\", 'up', 'each', \"doesn't\", 'we', \"shan't\", 'me', \"mightn't\", 'very', 'again', 'off', 'on', 'doesn', 'won', 'whom', 'both', 'mustn', 'those', 'has', 'i', 'they', 'the', 'here', 'how', 'themselves', \"shouldn't\", 'weren', 'having', 't', 'ma', 'now', 'only', 'be', 'her', 'and', 'until', 'once', 'do', 'by', 'just', 'am', 'its', 'myself', 'into', 'being', 'she', 'is', 'above', 'nor', 'if', 'can', 'shouldn', 'wouldn', 'of', 'own', 'so', 'these', 'down', 'have', 'isn', 'or', 'yourself', 'all', 'should', \"haven't\", \"didn't\", 'him', 'doing', 'hasn', 'under', 'a', 'from', \"should've\", 'this', \"hasn't\", 'herself', 're', 'against', \"couldn't\", 'shan', 'through', 'before', 'who', 'were', 'same', 'because', 'no', 'out', 've', 'below', 'for', 'what', 'during', 'couldn', 'while', \"she's\", 'after', 'then', 'too', \"you'll\", 'he', 'which', 'been', 'further', 'other', 'about', 'does', 'ourselves', 'theirs', 'an', 's', \"aren't\", \"wouldn't\", 'needn', 'you', 'o', 'll', 'are', 'don', 'himself', 'wasn', 'as', 'that', 'few', 'd', 'your', 'hadn', 'his', 'was', \"you're\", 'yourselves', 'than', 'at', 'didn', 'ain', \"weren't\", 'y', \"isn't\", 'between', 'their', \"wasn't\", 'why', 'over', 'when', 'any', 'will', \"you'd\", \"won't\", 'to', \"needn't\", 'our', 'there', 'itself', 'yours', \"mustn't\", 'more', 'm', 'most', 'mightn', 'them'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_word=set(stopwords.words('english'))\n",
    "print(stop_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would love to try or hear the sample audio your app can produce. I do not want to purchase, because I've purchased so many apps that say they do something and do not deliver.  \n",
      "\n",
      "Can you please add audio samples with text you've converted? I'd love to see the end results.\n",
      "\n",
      "Thanks!\n"
     ]
    }
   ],
   "source": [
    "test_file=open('C:\\\\Users\\\\best\\\\Downloads\\\\sample.txt')\n",
    "data_test=test_file.read()\n",
    "print(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "#### https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/\n",
    "#### https://medium.com/towards-artificial-intelligence/natural-language-processing-nlp-with-python-tutorial-for-beginners-1f54e610a1a0\n",
    "#### https://www.machinelearningplus.com/nlp/nlp-exercises/\n",
    "### Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'would', 'love', 'to', 'try', 'or', 'hear', 'the', 'sample', 'audio', 'your', 'app', 'can', 'produce', '.', 'I', 'do', 'not', 'want', 'to', 'purchase', ',', 'because', 'I', \"'ve\", 'purchased', 'so', 'many', 'apps', 'that', 'say', 'they', 'do', 'something', 'and', 'do', 'not', 'deliver', '.', 'Can', 'you', 'please', 'add', 'audio', 'samples', 'with', 'text', 'you', \"'ve\", 'converted', '?', 'I', \"'d\", 'love', 'to', 'see', 'the', 'end', 'results', '.', 'Thanks', '!']\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word=word_tokenize(data_test)\n",
    "print(word)\n",
    "print(len(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentance Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I would love to try or hear the sample audio your app can produce.', \"I do not want to purchase, because I've purchased so many apps that say they do something and do not deliver.\", \"Can you please add audio samples with text you've converted?\", \"I'd love to see the end results.\", 'Thanks!']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentence=sent_tokenize(data_test)\n",
    "print(sentence)\n",
    "print(len(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'would', 'love', 'to', 'try', 'or', 'hear', 'the', 'sample', 'audio', 'your', 'app', 'can', 'produce', 'i', 'do', 'not', 'want', 'to', 'purchase', 'because', 'i', 'purchased', 'so', 'many', 'apps', 'that', 'say', 'they', 'do', 'something', 'and', 'do', 'not', 'deliver', 'can', 'you', 'please', 'add', 'audio', 'samples', 'with', 'text', 'you', 'converted', 'i', 'love', 'to', 'see', 'the', 'end', 'results', 'thanks']\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "no_punc=[]\n",
    "for w in word:\n",
    "    if w.isalpha():\n",
    "        no_punc.append(w.lower())\n",
    "\n",
    "print(no_punc)\n",
    "print(len(no_punc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmer\n",
    "#### https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "#### https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/\n",
    "#### https://medium.com/towards-artificial-intelligence/natural-language-processing-nlp-with-python-tutorial-for-beginners-1f54e610a1a0\n",
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would love to tri or hear the sampl audio your app can produc . I do not want to purchas , becaus I 've purchas so mani app that say they do someth and do not deliv . can you pleas add audio sampl with text you 've convert ? I 'd love to see the end result . thank ! \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "porter.stem(data_test)\n",
    "stem_sentence=[]\n",
    "for w in word: \n",
    "    stem_sentence.append(porter.stem(w))\n",
    "    stem_sentence.append(\" \")\n",
    "\n",
    "\n",
    "print( \"\".join(stem_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i would lov to try or hear the sampl audio yo ap can produc . i do not want to purchas , becaus i 've purchas so many ap that say they do someth and do not del . can you pleas ad audio sampl with text you 've convert ? i 'd lov to see the end result . thank ! \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster=LancasterStemmer()\n",
    "lancaster.stem(data_test)\n",
    "lan_stem=[]\n",
    "for w in word:\n",
    "    lan_stem.append(lancaster.stem(w))\n",
    "    lan_stem.append(\" \")\n",
    "print( \"\".join(lan_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would love to tri or hear the sampl audio your app can produc . I do not want to purchas , becaus I 've purchas so mani app that say they do someth and do not deliv . can you pleas add audio sampl with text you 've convert ? I 'd love to see the end result . thank ! \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(data_test)\n",
    "lema_sentence=[]\n",
    "for w in word: \n",
    "    lema_sentence.append(lemmatizer.lemmatize(w))\n",
    "    lema_sentence.append(\" \")\n",
    "print(\"\".join(stem_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i want a green car . i would like to caring that perfectly.i simply going there for attending function \n"
     ]
    }
   ],
   "source": [
    "data=\"i want a green car. i would like to caring that perfectly.i simply going there for attending function\"\n",
    "d=word_tokenize(data)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(data)\n",
    "s=[]\n",
    "for w in d: \n",
    "    s.append(lemmatizer.lemmatize(w))\n",
    "    s.append(\" \")\n",
    "print(\"\".join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i want a green car . i would like to care that perfectly.i simpli go there for attend function \n"
     ]
    }
   ],
   "source": [
    "data=\"i want a green car. i would like to caring that perfectly.i simply going there for attending function\"\n",
    "d=word_tokenize(data)\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "porter.stem(data)\n",
    "\n",
    "s=[]\n",
    "for w in d: \n",
    "    s.append(porter.stem(w))\n",
    "    s.append(\" \")\n",
    "print(\"\".join(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorization\n",
    "#### https://medium.com/sanrusha-consultancy/natural-language-processing-nlp-and-countvectorizer-5571cf9205e4\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 2 1 2 1 1 3 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 3 1 2 1\n",
      "  1 1 2 1]]\n",
      "['add', 'and', 'app', 'apps', 'audio', 'because', 'can', 'converted', 'deliver', 'do', 'end', 'hear', 'love', 'many', 'not', 'or', 'please', 'produce', 'purchase', 'purchased', 'results', 'sample', 'samples', 'say', 'see', 'so', 'something', 'text', 'thanks', 'that', 'the', 'they', 'to', 'try', 've', 'want', 'with', 'would', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "data_corpus=[data_test]\n",
    "vocabulary=vectorizer.fit(data_corpus)\n",
    "X= vectorizer.transform(data_corpus)\n",
    "#print(X)\n",
    "print(X.toarray())\n",
    "print(vocabulary.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity\n",
    "#### https://www.machinelearningplus.com/nlp/nlp-exercises/\n",
    "#### https://www.machinelearningplus.com/nlp/cosine-similarity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.05241424 0.15384615 0.33968311 0.        ]\n",
      " [0.05241424 1.         0.05241424 0.07715167 0.        ]\n",
      " [0.15384615 0.05241424 1.         0.         0.        ]\n",
      " [0.33968311 0.07715167 0.         1.         0.        ]\n",
      " [0.         0.         0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for s in sentence:\n",
    "    documents.append(s)\n",
    "import pandas as pd\n",
    "\n",
    "# Create the Document Term Matrix\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(sparse_matrix, sparse_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "#### https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n",
    "#### https://www.guru99.com/word-embedding-word2vec.html\n",
    "#### https://shuzhanfan.github.io/2018/08/understanding-word2vec-and-doc2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('law', 0.9429217576980591), ('general', 0.9299753308296204), ('policy', 0.9271051287651062), ('agriculture', 0.9239358901977539), ('media', 0.9224634170532227), ('practice', 0.9188370108604431), ('discussion', 0.9165786504745483), ('board', 0.9128552079200745), ('Crean', 0.9115881323814392), ('Cooper', 0.9101938605308533)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\best\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.corpus import abc\n",
    "\n",
    "model= gensim.models.Word2Vec(abc.sents())\n",
    "X= list(model.wv.vocab)\n",
    "data=model.most_similar('science')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec\n",
    "#### https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "#### https://shuzhanfan.github.io/2018/08/understanding-word2vec-and-doc2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.34192031621932983),\n",
       " (3, 0.28647512197494507),\n",
       " (0, 0.24908685684204102),\n",
       " (2, 0.12915852665901184),\n",
       " (4, 0.10291960835456848)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Import packages\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "\n",
    "# Tokenization of each document\n",
    "tokenized_doc = []\n",
    "for s in sentence:\n",
    "    tokenized_doc.append(word_tokenize(s.lower()))\n",
    "tokenized_doc\n",
    "# Convert tokenized document into gensim formated tagged data\n",
    "tagged_data = [TaggedDocument(s, [i]) for i, s in enumerate(tokenized_doc)]\n",
    "tagged_data\n",
    "## Train doc2vec model\n",
    "modeldv = Doc2Vec(tagged_data, vector_size=20, window=2, min_count=1, workers=4, epochs = 100)\n",
    "## Print model vocabulary\n",
    "modeldv.wv.vocab\n",
    "\n",
    "# find most similar doc \n",
    "test_doc = word_tokenize(\"I would love to hear the songs from mobile\".lower())\n",
    "modeldv.docvecs.most_similar(positive=[modeldv.infer_vector(test_doc)],topn=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf\n",
    "#### https://www.geeksforgeeks.org/sklearn-feature-extraction-with-tf-idf/\n",
    "#### https://monkeylearn.com/blog/what-is-tf-idf/\n",
    "#### https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17)\t0.3007987756520354\n",
      "  (0, 6)\t0.2426825582129578\n",
      "  (0, 2)\t0.3007987756520354\n",
      "  (0, 39)\t0.3007987756520354\n",
      "  (0, 4)\t0.2426825582129578\n",
      "  (0, 21)\t0.3007987756520354\n",
      "  (0, 30)\t0.2426825582129578\n",
      "  (0, 11)\t0.3007987756520354\n",
      "  (0, 15)\t0.3007987756520354\n",
      "  (0, 33)\t0.3007987756520354\n",
      "  (0, 32)\t0.20144844572929596\n",
      "  (0, 12)\t0.2426825582129578\n",
      "  (0, 37)\t0.3007987756520354\n",
      "  (1, 8)\t0.1920967085936022\n",
      "  (1, 1)\t0.1920967085936022\n",
      "  (1, 26)\t0.1920967085936022\n",
      "  (1, 31)\t0.1920967085936022\n",
      "  (1, 23)\t0.1920967085936022\n",
      "  (1, 29)\t0.1920967085936022\n",
      "  (1, 3)\t0.1920967085936022\n",
      "  (1, 13)\t0.1920967085936022\n",
      "  (1, 25)\t0.1920967085936022\n",
      "  (1, 19)\t0.1920967085936022\n",
      "  (1, 34)\t0.154982414954085\n",
      "  (1, 5)\t0.1920967085936022\n",
      "  (1, 18)\t0.1920967085936022\n",
      "  (1, 35)\t0.1920967085936022\n",
      "  (1, 14)\t0.3841934171872044\n",
      "  (1, 9)\t0.5762901257808066\n",
      "  (1, 32)\t0.12864940454631404\n",
      "  (2, 7)\t0.2892451689348511\n",
      "  (2, 27)\t0.2892451689348511\n",
      "  (2, 36)\t0.2892451689348511\n",
      "  (2, 22)\t0.2892451689348511\n",
      "  (2, 0)\t0.2892451689348511\n",
      "  (2, 16)\t0.2892451689348511\n",
      "  (2, 38)\t0.5784903378697022\n",
      "  (2, 34)\t0.23336118106095702\n",
      "  (2, 6)\t0.23336118106095702\n",
      "  (2, 4)\t0.23336118106095702\n",
      "  (3, 20)\t0.4588147642793905\n",
      "  (3, 10)\t0.4588147642793905\n",
      "  (3, 24)\t0.4588147642793905\n",
      "  (3, 30)\t0.3701688628879373\n",
      "  (3, 32)\t0.3072735949186737\n",
      "  (3, 12)\t0.3701688628879373\n",
      "  (4, 28)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "corpus = sentence\n",
    "X = obj.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 4), ('to', 3), ('.', 3), ('do', 3), ('love', 2)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist(word)\n",
    "fdist.most_common(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
